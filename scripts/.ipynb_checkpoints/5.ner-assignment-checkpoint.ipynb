{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99797c0f",
   "metadata": {},
   "source": [
    "### NER Exercise\n",
    "\n",
    "Created by: Tan Poh Keam, Republic Polytechnic\n",
    "\n",
    "\n",
    "Techniques used to prepare the training and test data are taken from https://deepnote.com/@isaac-aderogba/Spacy-Food-Entities-LMLRnMOsQyGIUwvPLvVlsw .\n",
    "\n",
    "In this notebook, we'll be training spaCy to identify FOOD entities from given snippets of text.\n",
    "\n",
    "In the following cells, we will be generating the datasets using templates and food name fillers.\n",
    "\n",
    "Once we have the dataset, we will train a emptyspaCy NER model and add the custom FOOD entities. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7867a32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy import displacy\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c570fd3",
   "metadata": {},
   "source": [
    "**Prepare the training data**2\n",
    "From the the USDA's Branded Food's dataset, download the Apr 2021 Branded Food dataset from\n",
    "https://fdc.nal.usda.gov/download-datasets.html\n",
    "\n",
    "We will download the folder ../assets/ and unzip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11139267",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!wget -P ../assets https://fdc.nal.usda.gov/fdc-datasets/FoodData_Central_branded_food_csv_2021-04-28.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c811a1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l ../assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7290f474",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -o -q ../assets/FoodData_Central_branded_food_csv_2021-04-28.zip -d ../assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4948a666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 3433168\r\n",
      "-rw-r--r--  1 tanpohkeam  staff     148996 Nov  9  2020 Download & API Field Descriptions April 2021.pdf\r\n",
      "-rw-r--r--  1 tanpohkeam  staff  218215172 Apr 28 05:29 FoodData_Central_branded_food_csv_2021-04-28.zip\r\n",
      "-rw-r--r--  1 tanpohkeam  staff     402397 Jun 29 09:55 FoodData_Central_branded_food_csv_2021-04-28.zip.1\r\n",
      "-rw-r--r--  1 tanpohkeam  staff        140 Apr 27 05:25 all_downloaded_table_record_counts.csv\r\n",
      "-rw-r--r--  1 tanpohkeam  staff  499213246 Apr 27 05:15 branded_food.csv\r\n",
      "-rw-r--r--  1 tanpohkeam  staff       1173 Jun 27 10:36 dev.spacy\r\n",
      "-rw-r--r--  1 tanpohkeam  staff   92669707 Apr 27 05:16 food.csv\r\n",
      "-rw-r--r--  1 tanpohkeam  staff   63249964 Apr 27 05:16 food_attribute.csv\r\n",
      "-rw-r--r--  1 tanpohkeam  staff  780640628 Apr 27 05:19 food_nutrient.csv\r\n",
      "-rw-r--r--  1 tanpohkeam  staff   72102688 Apr 27 05:17 food_update_log_entry.csv\r\n",
      "-rw-r--r--  1 tanpohkeam  staff       1173 Jun 27 10:36 train.spacy\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l ../assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d50ec2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fdc_id</th>\n",
       "      <th>data_type</th>\n",
       "      <th>description</th>\n",
       "      <th>food_category_id</th>\n",
       "      <th>publication_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1105904</td>\n",
       "      <td>branded_food</td>\n",
       "      <td>WESSON Vegetable Oil 1 GAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-11-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1105905</td>\n",
       "      <td>branded_food</td>\n",
       "      <td>SWANSON BROTH BEEF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-11-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1105906</td>\n",
       "      <td>branded_food</td>\n",
       "      <td>CAMPBELL'S SLOW KETTLE SOUP CLAM CHOWDER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-11-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1105907</td>\n",
       "      <td>branded_food</td>\n",
       "      <td>CAMPBELL'S SLOW KETTLE SOUP CHEESE BROCCOLI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-11-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1105908</td>\n",
       "      <td>branded_food</td>\n",
       "      <td>SWANSON BROTH CHICKEN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-11-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fdc_id     data_type                                  description  \\\n",
       "0  1105904  branded_food                   WESSON Vegetable Oil 1 GAL   \n",
       "1  1105905  branded_food                           SWANSON BROTH BEEF   \n",
       "2  1105906  branded_food     CAMPBELL'S SLOW KETTLE SOUP CLAM CHOWDER   \n",
       "3  1105907  branded_food  CAMPBELL'S SLOW KETTLE SOUP CHEESE BROCCOLI   \n",
       "4  1105908  branded_food                        SWANSON BROTH CHICKEN   \n",
       "\n",
       "   food_category_id publication_date  \n",
       "0               NaN       2020-11-13  \n",
       "1               NaN       2020-11-13  \n",
       "2               NaN       2020-11-13  \n",
       "3               NaN       2020-11-13  \n",
       "4               NaN       2020-11-13  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "path_to_datafile =\"../assets/food.csv\"\n",
    "food_df = pd.read_csv(path_to_datafile)\n",
    "\n",
    "# print row and column information\n",
    "food_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9755e615",
   "metadata": {},
   "source": [
    "In the dataframe, the column 'Description\" contains the food item description.\n",
    "For simplicity, we will only choose food description that DOES NOT contains special characters, and we retain rows with food descriptions of 1, 2 , or 3 words only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "252f7090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "food items total : 40508\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1          swanson broth beef\n",
       "4       swanson broth chicken\n",
       "25    pepperidge farm cookies\n",
       "35      pepperidge farm bread\n",
       "42    swanson broth vegetable\n",
       "Name: description, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "foods = food_df[food_df[\"description\"].str.contains(\"[^a-zA-Z ]\") == False][\"description\"].apply(lambda food: food.lower())\n",
    "\n",
    "# filter out foods with more than 3 words, drop any duplicates\n",
    "foods = foods[foods.str.split().apply(len) <= 3].drop_duplicates()\n",
    "\n",
    "# print the remaining size\n",
    "print(\"food items total :\" , foods.size)\n",
    "\n",
    "# show first 5 food items\n",
    "foods.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc96684",
   "metadata": {},
   "source": [
    "Prepares seperate list of foods entities with one word, two words and three words.\n",
    "This will be need to simplify the way we create the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ae44409",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_worded_foods = foods[foods.str.split().apply(len) == 1]\n",
    "two_worded_foods = foods[foods.str.split().apply(len) == 2]\n",
    "three_worded_foods = foods[foods.str.split().apply(len) == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acfa641f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238    blueberries\n",
       "314       crackers\n",
       "501          dills\n",
       "634          adobo\n",
       "687        gummies\n",
       "Name: description, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_worded_foods.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dafc2a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182       walnut butter\n",
       "195       tita crackers\n",
       "206      teriyaki sauce\n",
       "214      dessert shells\n",
       "236    italian dressing\n",
       "Name: description, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_worded_foods.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e193e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1          swanson broth beef\n",
       "4       swanson broth chicken\n",
       "25    pepperidge farm cookies\n",
       "35      pepperidge farm bread\n",
       "42    swanson broth vegetable\n",
       "Name: description, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_worded_foods.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a494d7",
   "metadata": {},
   "source": [
    "At this point, we want to create different placeholders templates that we can insert our food entities into. \n",
    "The training sentences should cater to mentioning one, two or three food items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94c83f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_templates = [\n",
    "    \"I ate my {}.\",\n",
    "    \"I'm eating a {}.\",\n",
    "    \"I just ate a {}.\",\n",
    "    \"I only ate the {}.\",\n",
    "    \"I'm done eating a {}\",\n",
    "    \"I've already eaten a {}.\",\n",
    "    \"I just finished my {}.\",\n",
    "    \"When I was having lunch I ate a {}.\",\n",
    "    \"I had a {} and a {} today\",\n",
    "    \"I ate a {} and a {} for lunch\",\n",
    "    \"I made a {} and {} for lunch\",\n",
    "    \"I ate {} and {} just now\",\n",
    "    \"today I ate a {} and a {} for lunch\",\n",
    "    \"I had {} with my husband last night\",\n",
    "    \"I brought you some {} on my birthday\",\n",
    "    \"I made {} for yesterday's dinner\",\n",
    "    \"last night, a {} was sent to me via Grabfood\",\n",
    "    \"I had {} yesterday and I'd like to eat it anyway\",\n",
    "    \"I ate a couple of {} last night\",\n",
    "    \"I had some {} at dinner last night\",\n",
    "    \"Last night, I ordered some {} as I was hungry\",\n",
    "    \"I made a {} last night\",\n",
    "    \"I had a bowl of {} with {} and I wanted to go to the mall today\",\n",
    "    \"I brought a basket of {} for breakfast this morning\",\n",
    "    \"I had a bowl of {} just now\",\n",
    "    \"I ate a {} with {} in the morning\",\n",
    "    \"I made a bowl of {} for my breakfast\",\n",
    "    \"There's {} for breakfast in the bowl this morning\",\n",
    "    \"I made a bowl of {} thius morning\",\n",
    "    \"I decided to have some {} as a little bonus\",\n",
    "    \"I decided to enjoy some {} to relax\",\n",
    "    \"I've decided to have some {} for dessert\",\n",
    "    \"I had a {}, a {} and {} at home\",\n",
    "    \"I took a {}, {} and {} on the weekend\",\n",
    "    \"I ate a {} with {} and {} just now\",\n",
    "    \"Last night, I ate an {} with {} and {} with my girl friend\",\n",
    "    \"I tasted some {}, {} and {} at the office\",\n",
    "    \"There's a basket of {}, {} and {} that I consumed\",\n",
    "    \"I devoured a {}, {} and {} for dinner\",\n",
    "    \"I've already had a bag of {}, {} and {} from the fridge\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d74bd0",
   "metadata": {},
   "source": [
    "We need to prepare the data into the following format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56aa1f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "data = [\n",
    "    (\"I love chicken\", [(7, 13, \"FOOD\")]),\n",
    "    (\"We ordered Sushi thorugh Grabfood\", [(11, 16, \"FOOD\")]),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20373ace",
   "metadata": {},
   "source": [
    "Next, we will generate the training data and prepare them into the desired format.\n",
    "#### Do Not Modify The Next Block of Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b22d54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DO NOT MOIFY THIS BLOCK OF CODES.\n",
    "\n",
    "## create dictionaries to store the generated food combinations. \n",
    "## Do note that one_food != one_worded_food. one_food == \"barbecue sauce\", one_worded_food == \"sauce\"\n",
    "## Rather, one_food refers to sentences that talks about one type of food,regardless of the number of words\n",
    "## in the food item.\n",
    "\n",
    " # reset the data list\n",
    "\n",
    "\n",
    "data =[]  \n",
    "# the pattern to replace from the template sentences\n",
    "pattern_to_replace = \"{}\"\n",
    "\n",
    "# shuffle the data before starting\n",
    "foods = foods.sample(frac=1)\n",
    "\n",
    "# the count that helps us decide when to break from the for loop\n",
    "food_entity_count = foods.size - 1\n",
    "\n",
    "# start the while loop, ensure we don't get an index out of bounds error\n",
    "while food_entity_count >= 2:\n",
    "    entities = []\n",
    "\n",
    "    # pick a random food template\n",
    "    sentence = food_templates[random.randint(0, len(food_templates) - 1)]\n",
    "\n",
    "    # find out how many braces \"{}\" need to be replaced in the template\n",
    "    matches = re.findall(pattern_to_replace, sentence)\n",
    "\n",
    "    # for each brace, replace with a food entity from the shuffled food data\n",
    "    for match in matches:\n",
    "        food = foods.iloc[food_entity_count]\n",
    "        food_entity_count -= 1\n",
    "\n",
    "        # replace the pattern, but then find the match of the food entity we just inserted\n",
    "        sentence = sentence.replace(match, food, 1)\n",
    "        match_span = re.search(food, sentence).span()\n",
    "\n",
    "        # use that match to find the index positions of the food entity in the sentence, append\n",
    "        entities.append((match_span[0], match_span[1], \"FOOD\"))\n",
    "\n",
    "    # append the sentence and the position of the entities to the correct dictionary and array\n",
    "    data.append((sentence, {\"entities\": entities}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01ca22d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of the data contents\n",
      "\n",
      "('I brought you some sweetened diced cranberries on my birthday', {'entities': [(19, 46, 'FOOD')]})\n",
      "('I decided to enjoy some cheddar to relax', {'entities': [(24, 31, 'FOOD')]})\n",
      "('I tasted some spicy pickled mushrooms, homestyle cranberry sauce and holiday milk at the office', {'entities': [(14, 37, 'FOOD'), (39, 64, 'FOOD'), (69, 81, 'FOOD')]})\n",
      "('When I was having lunch I ate a turkey stock concentrate.', {'entities': [(32, 56, 'FOOD')]})\n",
      "('I just finished my hearty crispbread.', {'entities': [(19, 36, 'FOOD')]})\n"
     ]
    }
   ],
   "source": [
    "print ('Sample of the data contents\\n')\n",
    "for d in data[0:5]:\n",
    "    print (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bc2dc0",
   "metadata": {},
   "source": [
    "The next function is used to convert the data (prepared in the format above) to Spacy Docbin format.\n",
    "**Do not change the next block of codes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eef02110",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Do NOT CHANGE THIS BLOCK OF CODES.\n",
    "\n",
    "nlp = spacy.blank(\"en\") # load a new spacy model\n",
    "\n",
    "def  create_spacy3_training_data(DATA):\n",
    "    db = DocBin() # create a DocBin object\n",
    "    for text, annot in tqdm(DATA): # data in previous format\n",
    "       doc = nlp.make_doc(text) # create doc object from text\n",
    "       ents = []\n",
    "       for start, end, label in annot[\"entities\"]: # add character indexes\n",
    "           span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "           if span is None:\n",
    "               print(\"Skipping entity\")\n",
    "           else:\n",
    "              ents.append(span)\n",
    "       doc.ents = ents # label the text with the ents\n",
    "       db.add(doc)\n",
    "    return (db)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615fddfb",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "\n",
    "Typically, datasets needs to split into test and validation sets.\n",
    "The dataset that was generated is huge (25K).\n",
    "As a first step, you can use the slice it such that 1000 as training data, and the next 200 as validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e66ca816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to slice the first 10, use the following:\n",
    "\n",
    "sample_slice = data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16d5d8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# your codes\n",
    "TRAIN_DATA= ?????\n",
    "TEST_DATA= ??\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c7bb8f",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "From here onwards, convert the training data sets and validation data sets to Spacy Docbin format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77477bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your codes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cde3f2f",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "Run the spacy CLI commands with suitable parameters to start the NER training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3713c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your codes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37025d6f",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "Reload your new model. \n",
    "Test your new model against unseen sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b966c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## your codes\n",
    "\n",
    "sentences  = [ 'I ate wantan noodles and kopi for lunch', \n",
    "               'Last night, I ate instant noodles ',\n",
    "               'I plan to eat beef steak and ice-cream for dinner.',\n",
    "               'I ordered a chicken steak, beer and cheesecake for dinner']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a4e522",
   "metadata": {},
   "source": [
    "### Solutions for Task 1, Task 2, and Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be281753",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 4470.60it/s]\n"
     ]
    }
   ],
   "source": [
    "train_db = create_spacy3_training_data(TRAIN_DATA)\n",
    "train_db.to_disk(\"./food_train.spacy\") # save the docbin object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04d90b83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 2333.34it/s]\n"
     ]
    }
   ],
   "source": [
    "test_db = create_spacy3_training_data(TEST_DATA[0:100])\n",
    "test_db.to_disk(\"./food_dev.spacy\") # save the docbin object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3b04d7b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2021-06-29 10:08:06,585] [INFO] Set up nlp object from config\n",
      "[2021-06-29 10:08:06,589] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2021-06-29 10:08:06,590] [INFO] Created vocabulary\n",
      "[2021-06-29 10:08:06,590] [INFO] Finished initializing nlp object\n",
      "[2021-06-29 10:08:07,071] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "/Users/tanpohkeam/miniforge3/envs/spacy/lib/python3.8/site-packages/thinc/layers/layernorm.py:32: RuntimeWarning: divide by zero encountered in reciprocal\n",
      "  d_xhat = N * dY - sum_dy - dist * var ** (-1.0) * sum_dy_dist\n",
      "  0       0          0.00     48.50    0.00    0.00    0.00    0.00\n",
      "  0      50         15.43   1299.79   59.65   58.96   60.36    0.60\n",
      "  0     100         13.13    166.88   97.30   98.78   95.86    0.97\n",
      "  1     150          6.71     24.69   97.35   97.06   97.63    0.97\n",
      "  1     200         13.57     28.88   97.91   98.80   97.04    0.98\n",
      "  2     250         18.11     25.80   97.63   97.63   97.63    0.98\n",
      "  2     300         11.03     11.19   97.63   97.63   97.63    0.98\n",
      "  3     350         10.65     13.90   96.72   97.59   95.86    0.97\n",
      "  3     400          8.46      8.52   99.11   99.40   98.82    0.99\n",
      "  4     450         10.83      9.35   99.11   99.40   98.82    0.99\n",
      "  5     500          6.85      6.40   99.11   99.40   98.82    0.99\n",
      "  5     550          0.04      0.03   98.53   98.24   98.82    0.99\n",
      "  6     600          2.71      3.11   99.12   98.82   99.41    0.99\n",
      "  7     650          0.12      0.08   97.06   96.49   97.63    0.97\n",
      "  8     700          0.15      0.21   97.62   98.20   97.04    0.98\n",
      "  8     750          3.12      1.86   98.52   98.81   98.22    0.99\n",
      "  9     800          0.00      0.00   98.52   98.81   98.22    0.99\n",
      " 10     850          9.21      6.02   97.04   97.04   97.04    0.97\n",
      " 11     900          0.21      0.15   98.52   98.81   98.22    0.99\n",
      " 12     950          4.42      3.21   98.52   98.81   98.22    0.99\n",
      " 13    1000         14.17      7.06   98.52   98.81   98.22    0.99\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "output/model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train config.cfg --output ./output --paths.train ./food_train.spacy --paths.dev food_dev.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e42e0942",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\r\n",
      "drwxr-xr-x  8 tanpohkeam  staff  256 Jun 29 10:08 \u001b[34mmodel-best\u001b[m\u001b[m\r\n",
      "drwxr-xr-x  8 tanpohkeam  staff  256 Jun 29 10:08 \u001b[34mmodel-last\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c9513a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I ate \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    wantan noodles\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FOOD</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    kopi\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FOOD</span>\n",
       "</mark>\n",
       " for lunch</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Last night, I ate \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    instant noodles\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FOOD</span>\n",
       "</mark>\n",
       " </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I plan to eat \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    beef steak\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FOOD</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ice-cream\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FOOD</span>\n",
       "</mark>\n",
       " for dinner.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I ordered a \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    chicken steak\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FOOD</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    beer\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FOOD</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    cheesecake\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FOOD</span>\n",
       "</mark>\n",
       " for dinner</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_nlp = spacy.load(\"./output/model-best\")\n",
    "\n",
    "sentences  = [ 'I ate wantan noodles and kopi for lunch', \n",
    "               'Last night, I ate instant noodles ',\n",
    "               'I plan to eat beef steak and ice-cream for dinner.',\n",
    "               'I ordered a chicken steak, beer and cheesecake for dinner']\n",
    "\n",
    "for s in sentences:\n",
    "   doc = best_nlp(s)\n",
    "   displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b261d9fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
