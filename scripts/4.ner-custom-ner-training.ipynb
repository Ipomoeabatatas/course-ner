{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99797c0f",
   "metadata": {},
   "source": [
    "### NER Exercise\n",
    "\n",
    "Created by: Tan Poh Keam, Republic Polytechnic\n",
    "\n",
    "\n",
    "Techniques used to prepare the training and test data are taken from https://deepnote.com/@isaac-aderogba/Spacy-Food-Entities-LMLRnMOsQyGIUwvPLvVlsw .\n",
    "\n",
    "In this notebook, we'll be training spaCy to identify FOOD entities from given snippets of text.\n",
    "\n",
    "In the following cells, we will be generating the datasets using templates and food name fillers.\n",
    "\n",
    "Once we have the dataset, we will train a emptyspaCy NER model and add the custom FOOD entities. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7867a32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy import displacy\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c570fd3",
   "metadata": {},
   "source": [
    "**Prepare the training data**2\n",
    "From the the USDA's Branded Food's dataset, download the Apr 2021 Branded Food dataset from\n",
    "https://fdc.nal.usda.gov/download-datasets.html\n",
    "\n",
    "We will download the folder ../assets/ and unzip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11139267",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-06-30 22:15:59--  https://fdc.nal.usda.gov/fdc-datasets/FoodData_Central_branded_food_csv_2021-04-28.zip\n",
      "Resolving fdc.nal.usda.gov (fdc.nal.usda.gov)... 199.136.16.176\n",
      "Connecting to fdc.nal.usda.gov (fdc.nal.usda.gov)|199.136.16.176|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 218215172 (208M) [application/zip]\n",
      "Saving to: ‘../assets/FoodData_Central_branded_food_csv_2021-04-28.zip’\n",
      "\n",
      "FoodData_Central_br  47%[========>           ]  98.04M   117KB/s    in 7m 39s  \n",
      "\n",
      "2021-06-30 22:23:39 (219 KB/s) - Connection closed at byte 102801925. Retrying.\n",
      "\n",
      "--2021-06-30 22:23:40--  (try: 2)  https://fdc.nal.usda.gov/fdc-datasets/FoodData_Central_branded_food_csv_2021-04-28.zip\n",
      "Connecting to fdc.nal.usda.gov (fdc.nal.usda.gov)|199.136.16.176|:443... connected.\n",
      "HTTP request sent, awaiting response... 206 Partial Content\n",
      "Length: 218215172 (208M), 115413247 (110M) remaining [application/zip]\n",
      "Saving to: ‘../assets/FoodData_Central_branded_food_csv_2021-04-28.zip’\n",
      "\n",
      "FoodData_Central_br 100%[+++++++++==========>] 208.11M   491KB/s    in 4m 6s   \n",
      "\n",
      "2021-06-30 22:27:48 (458 KB/s) - ‘../assets/FoodData_Central_branded_food_csv_2021-04-28.zip’ saved [218215172/218215172]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -P ../assets https://fdc.nal.usda.gov/fdc-datasets/FoodData_Central_branded_food_csv_2021-04-28.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c811a1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l ../assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7290f474",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -o -q ../assets/FoodData_Central_branded_food_csv_2021-04-28.zip -d ../assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4948a666",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls -l ../assets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d50ec2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fdc_id</th>\n",
       "      <th>data_type</th>\n",
       "      <th>description</th>\n",
       "      <th>food_category_id</th>\n",
       "      <th>publication_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1105904</td>\n",
       "      <td>branded_food</td>\n",
       "      <td>WESSON Vegetable Oil 1 GAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-11-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1105905</td>\n",
       "      <td>branded_food</td>\n",
       "      <td>SWANSON BROTH BEEF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-11-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1105906</td>\n",
       "      <td>branded_food</td>\n",
       "      <td>CAMPBELL'S SLOW KETTLE SOUP CLAM CHOWDER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-11-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1105907</td>\n",
       "      <td>branded_food</td>\n",
       "      <td>CAMPBELL'S SLOW KETTLE SOUP CHEESE BROCCOLI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-11-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1105908</td>\n",
       "      <td>branded_food</td>\n",
       "      <td>SWANSON BROTH CHICKEN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-11-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fdc_id     data_type                                  description  \\\n",
       "0  1105904  branded_food                   WESSON Vegetable Oil 1 GAL   \n",
       "1  1105905  branded_food                           SWANSON BROTH BEEF   \n",
       "2  1105906  branded_food     CAMPBELL'S SLOW KETTLE SOUP CLAM CHOWDER   \n",
       "3  1105907  branded_food  CAMPBELL'S SLOW KETTLE SOUP CHEESE BROCCOLI   \n",
       "4  1105908  branded_food                        SWANSON BROTH CHICKEN   \n",
       "\n",
       "   food_category_id publication_date  \n",
       "0               NaN       2020-11-13  \n",
       "1               NaN       2020-11-13  \n",
       "2               NaN       2020-11-13  \n",
       "3               NaN       2020-11-13  \n",
       "4               NaN       2020-11-13  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "path_to_datafile =\"../assets/food.csv\"\n",
    "food_df = pd.read_csv(path_to_datafile)\n",
    "\n",
    "# print row and column information\n",
    "food_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9755e615",
   "metadata": {},
   "source": [
    "In the dataframe, the column 'Description\" contains the food item description.\n",
    "For simplicity, we will only choose food description that DOES NOT contains special characters, and we retain rows with food descriptions of 1, 2 , or 3 words only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "252f7090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "food items total : 40508\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1          swanson broth beef\n",
       "4       swanson broth chicken\n",
       "25    pepperidge farm cookies\n",
       "35      pepperidge farm bread\n",
       "42    swanson broth vegetable\n",
       "Name: description, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "foods = food_df[food_df[\"description\"].str.contains(\"[^a-zA-Z ]\") == False][\"description\"].apply(lambda food: food.lower())\n",
    "\n",
    "# filter out foods with more than 3 words, drop any duplicates\n",
    "foods = foods[foods.str.split().apply(len) <= 3].drop_duplicates()\n",
    "\n",
    "# print the remaining size\n",
    "print(\"food items total :\" , foods.size)\n",
    "\n",
    "# show first 5 food items\n",
    "foods.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc96684",
   "metadata": {},
   "source": [
    "Prepares seperate list of foods entities with one word, two words and three words.\n",
    "This will be need to simplify the way we create the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ae44409",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_worded_foods = foods[foods.str.split().apply(len) == 1]\n",
    "two_worded_foods = foods[foods.str.split().apply(len) == 2]\n",
    "three_worded_foods = foods[foods.str.split().apply(len) == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acfa641f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238    blueberries\n",
       "314       crackers\n",
       "501          dills\n",
       "634          adobo\n",
       "687        gummies\n",
       "Name: description, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_worded_foods.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dafc2a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182       walnut butter\n",
       "195       tita crackers\n",
       "206      teriyaki sauce\n",
       "214      dessert shells\n",
       "236    italian dressing\n",
       "Name: description, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_worded_foods.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e193e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1          swanson broth beef\n",
       "4       swanson broth chicken\n",
       "25    pepperidge farm cookies\n",
       "35      pepperidge farm bread\n",
       "42    swanson broth vegetable\n",
       "Name: description, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_worded_foods.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d74bd0",
   "metadata": {},
   "source": [
    "We need to prepare the data into the following format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56aa1f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "data = [\n",
    "    (\"I love chicken\", [(7, 13, \"FOOD\")]),\n",
    "    (\"We ordered Sushi thorugh Grabfood\", [(11, 16, \"FOOD\")]),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a494d7",
   "metadata": {},
   "source": [
    "At this point, we want to create different placeholders templates that we can insert our food entities into. \n",
    "The training sentences should cater to mentioning one, two or three food items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94c83f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_templates = [\n",
    "    \"I ate my {}.\",\n",
    "    \"I'm eating a {}.\",\n",
    "    \"I just ate a {}.\",\n",
    "    \"I only ate the {}.\",\n",
    "    \"I'm done eating a {}\",\n",
    "    \"I've already eaten a {}.\",\n",
    "    \"I just finished my {}.\",\n",
    "    \"When I was having lunch I ate a {}.\",\n",
    "    \"I had a {} and a {} today\",\n",
    "    \"I ate a {} and a {} for lunch\",\n",
    "    \"I made a {} and {} for lunch\",\n",
    "    \"I ate {} and {} just now\",\n",
    "    \"today I ate a {} and a {} for lunch\",\n",
    "    \"I had {} with my husband last night\",\n",
    "    \"I brought you some {} on my birthday\",\n",
    "    \"I made {} for yesterday's dinner\",\n",
    "    \"last night, a {} was sent to me via Grabfood\",\n",
    "    \"I had {} yesterday and I'd like to eat it anyway\",\n",
    "    \"I ate a couple of {} last night\",\n",
    "    \"I had some {} at dinner last night\",\n",
    "    \"Last night, I ordered some {} as I was hungry\",\n",
    "    \"I made a {} last night\",\n",
    "    \"I had a bowl of {} with {} and I wanted to go to the mall today\",\n",
    "    \"I brought a basket of {} for breakfast this morning\",\n",
    "    \"I had a bowl of {} just now\",\n",
    "    \"I ate a {} with {} in the morning\",\n",
    "    \"I made a bowl of {} for my breakfast\",\n",
    "    \"There's {} for breakfast in the bowl this morning\",\n",
    "    \"I made a bowl of {} thius morning\",\n",
    "    \"I decided to have some {} as a little bonus\",\n",
    "    \"I decided to enjoy some {} to relax\",\n",
    "    \"I've decided to have some {} for dessert\",\n",
    "    \"I had a {}, a {} and {} at home\",\n",
    "    \"I took a {}, {} and {} on the weekend\",\n",
    "    \"I ate a {} with {} and {} just now\",\n",
    "    \"Last night, I ate an {} with {} and {} with my girl friend\",\n",
    "    \"I tasted some {}, {} and {} at the office\",\n",
    "    \"There's a basket of {}, {} and {} that I consumed\",\n",
    "    \"I devoured a {}, {} and {} for dinner\",\n",
    "    \"I've already had a bag of {}, {} and {} from the fridge\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20373ace",
   "metadata": {},
   "source": [
    "Next, we will generate the training data and prepare them into the desired format.\n",
    "#### Do Not Modify The Next Block of Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b22d54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DO NOT MOIFY THIS BLOCK OF CODES.\n",
    "\n",
    "## create dictionaries to store the generated food combinations. \n",
    "## Do note that one_food != one_worded_food. one_food == \"barbecue sauce\", one_worded_food == \"sauce\"\n",
    "## Rather, one_food refers to sentences that talks about one type of food,regardless of the number of words\n",
    "## in the food item.\n",
    "\n",
    " # reset the data list\n",
    "\n",
    "\n",
    "data =[]  \n",
    "# the pattern to replace from the template sentences\n",
    "pattern_to_replace = \"{}\"\n",
    "\n",
    "# shuffle the data before starting\n",
    "foods = foods.sample(frac=1)\n",
    "\n",
    "# the count that helps us decide when to break from the for loop\n",
    "food_entity_count = foods.size - 1\n",
    "\n",
    "# start the while loop, ensure we don't get an index out of bounds error\n",
    "while food_entity_count >= 2:\n",
    "    entities = []\n",
    "\n",
    "    # pick a random food template\n",
    "    sentence = food_templates[random.randint(0, len(food_templates) - 1)]\n",
    "\n",
    "    # find out how many braces \"{}\" need to be replaced in the template\n",
    "    matches = re.findall(pattern_to_replace, sentence)\n",
    "\n",
    "    # for each brace, replace with a food entity from the shuffled food data\n",
    "    for match in matches:\n",
    "        food = foods.iloc[food_entity_count]\n",
    "        food_entity_count -= 1\n",
    "\n",
    "        # replace the pattern, but then find the match of the food entity we just inserted\n",
    "        sentence = sentence.replace(match, food, 1)\n",
    "        match_span = re.search(food, sentence).span()\n",
    "\n",
    "        # use that match to find the index positions of the food entity in the sentence, append\n",
    "        entities.append((match_span[0], match_span[1], \"FOOD\"))\n",
    "\n",
    "    # append the sentence and the position of the entities to the correct dictionary and array\n",
    "    data.append((sentence, {\"entities\": entities}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01ca22d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of the data contents\n",
      "\n",
      "('I brought you some marshmallow bars on my birthday', {'entities': [(19, 35, 'FOOD')]})\n",
      "('I ate a special corn tortillas with creamy horseradish in the morning', {'entities': [(8, 30, 'FOOD'), (36, 54, 'FOOD')]})\n",
      "('I made a bowl of seafood salad select thius morning', {'entities': [(17, 37, 'FOOD')]})\n",
      "('When I was having lunch I ate a simply buttermilk biscuit.', {'entities': [(32, 57, 'FOOD')]})\n",
      "('I made a bowl of crab thius morning', {'entities': [(17, 21, 'FOOD')]})\n"
     ]
    }
   ],
   "source": [
    "print ('Sample of the data contents\\n')\n",
    "for d in data[0:5]:\n",
    "    print (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bc2dc0",
   "metadata": {},
   "source": [
    "The next function is used to convert the data (prepared in the format above) to Spacy Docbin format.\n",
    "**Do not change the next block of codes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eef02110",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Do NOT CHANGE THIS BLOCK OF CODES.\n",
    "\n",
    "nlp = spacy.blank(\"en\") # load a new spacy model\n",
    "\n",
    "def  create_spacy3_training_data(DATA):\n",
    "    db = DocBin() # create a DocBin object\n",
    "    for text, annot in tqdm(DATA): # data in previous format\n",
    "       doc = nlp.make_doc(text) # create doc object from text\n",
    "       ents = []\n",
    "       for start, end, label in annot[\"entities\"]: # add character indexes\n",
    "           span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "           if span is None:\n",
    "               print(\"Skipping entity\")\n",
    "           else:\n",
    "              ents.append(span)\n",
    "       doc.ents = ents # label the text with the ents\n",
    "       db.add(doc)\n",
    "    return (db)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615fddfb",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "\n",
    "Typically, datasets needs to split into test and validation sets.\n",
    "The dataset that was generated is huge (25K).\n",
    "As a first step, you can use the slice it such that 1000 as training data, and the next 200 as validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e66ca816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to slice the first 10, use the following:\n",
    "\n",
    "sample_slice = data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16d5d8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# your codes\n",
    "TRAIN_DATA= data[0:1000]\n",
    "TEST_DATA= data[1000:1200]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c7bb8f",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "From here onwards, convert the training data sets and validation data sets to Spacy Docbin format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77477bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your codes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cde3f2f",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "Run the spacy CLI commands with suitable parameters to start the NER training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3713c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your codes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37025d6f",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "Reload your new model. \n",
    "Test your new model against unseen sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b966c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sentences  = [ 'I ate wantan noodles and kopi for lunch', \n",
    "               'Last night, I ate instant noodles ',\n",
    "               'I plan to eat beef steak and ice-cream for dinner.',\n",
    "               'I ordered a chicken steak, beer and cheesecake for dinner']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31272ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a4e522",
   "metadata": {},
   "source": [
    "### Solutions for Task 1, Task 2, and Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be281753",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 4801.16it/s]\n"
     ]
    }
   ],
   "source": [
    "train_db = create_spacy3_training_data(TRAIN_DATA)\n",
    "train_db.to_disk(\"../assets/food_train.spacy\") # save the docbin object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04d90b83",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 2342.91it/s]\n"
     ]
    }
   ],
   "source": [
    "test_db = create_spacy3_training_data(TEST_DATA[0:100])\n",
    "test_db.to_disk(\"../assets/food_dev.spacy\") # save the docbin object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b3b04d7b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2021-06-30 22:29:57,505] [INFO] Set up nlp object from config\n",
      "[2021-06-30 22:29:57,509] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2021-06-30 22:29:57,511] [INFO] Created vocabulary\n",
      "[2021-06-30 22:29:57,511] [INFO] Finished initializing nlp object\n",
      "[2021-06-30 22:29:57,987] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     48.17    0.00    0.00    0.00    0.00\n",
      "/Users/tanpohkeam/miniforge3/envs/spacy/lib/python3.8/site-packages/thinc/layers/layernorm.py:32: RuntimeWarning: divide by zero encountered in reciprocal\n",
      "  d_xhat = N * dY - sum_dy - dist * var ** (-1.0) * sum_dy_dist\n",
      "  0      50         15.84   1274.98   81.53   80.50   82.58    0.82\n",
      "  0     100         14.78    150.81   98.38   98.70   98.06    0.98\n",
      "  1     150          9.74     44.35   99.35   99.35   99.35    0.99\n",
      "  1     200         13.41     27.97   99.35   99.35   99.35    0.99\n",
      "  2     250          5.79     11.85   99.35   99.35   99.35    0.99\n",
      "  2     300          5.19      5.60  100.00  100.00  100.00    1.00\n",
      "  3     350          5.05      4.11  100.00  100.00  100.00    1.00\n",
      "  4     400         14.28     11.99  100.00  100.00  100.00    1.00\n",
      "  4     450          2.68      3.91  100.00  100.00  100.00    1.00\n",
      "  5     500          1.89      2.00  100.00  100.00  100.00    1.00\n",
      "  6     550          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "  6     600          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "  7     650          0.43      0.28  100.00  100.00  100.00    1.00\n",
      "  8     700          2.39      1.27   99.03   99.35   98.71    0.99\n",
      "  9     750          4.57      5.98  100.00  100.00  100.00    1.00\n",
      " 10     800          0.00      0.00  100.00  100.00  100.00    1.00\n",
      " 11     850          0.00      0.00  100.00  100.00  100.00    1.00\n",
      " 12     900          0.00      0.00  100.00  100.00  100.00    1.00\n",
      " 13     950          0.01      0.00  100.00  100.00  100.00    1.00\n",
      " 14    1000          0.00      0.00   99.03   99.35   98.71    0.99\n",
      " 15    1050          0.00      0.00   99.03   99.35   98.71    0.99\n",
      " 16    1100          0.04      0.02  100.00  100.00  100.00    1.00\n",
      " 18    1150          0.00      0.00  100.00  100.00  100.00    1.00\n",
      " 19    1200          0.00      0.00  100.00  100.00  100.00    1.00\n",
      " 20    1250          0.00      0.00  100.00  100.00  100.00    1.00\n",
      " 22    1300          0.00      0.00  100.00  100.00  100.00    1.00\n",
      " 23    1350          0.00      0.00  100.00  100.00  100.00    1.00\n",
      " 25    1400          0.00      0.00  100.00  100.00  100.00    1.00\n",
      " 27    1450          2.77      1.30  100.00  100.00  100.00    1.00\n",
      " 29    1500          1.33      1.08   99.68  100.00   99.35    1.00\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "../output/model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train ../configs/config.cfg --output ../output --paths.train ../assets/food_train.spacy --paths.dev ../assets/food_dev.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e42e0942",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\r\n",
      "drwxr-xr-x  8 tanpohkeam  staff  256 Jun 30 22:29 \u001b[34mmodel-best\u001b[m\u001b[m\r\n",
      "drwxr-xr-x  8 tanpohkeam  staff  256 Jun 30 22:29 \u001b[34mmodel-last\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l ../output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c9513a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I ate \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    wantan noodles\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FOOD</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    kopi\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FOOD</span>\n",
       "</mark>\n",
       " for lunch</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Last night, I ate \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    instant noodles\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FOOD</span>\n",
       "</mark>\n",
       " </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I plan to eat \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    beef steak\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FOOD</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ice-cream\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FOOD</span>\n",
       "</mark>\n",
       " for dinner.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I ordered a \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    roti-john\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FOOD</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    bundung\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FOOD</span>\n",
       "</mark>\n",
       "  for takeaway</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_nlp = spacy.load(\"../output/model-best\")\n",
    "\n",
    "sentences  = [ 'I ate wantan noodles and kopi for lunch', \n",
    "               'Last night, I ate instant noodles ',\n",
    "               'I plan to eat beef steak and ice-cream for dinner.',\n",
    "               'I ordered a roti-john and bundung  for takeaway']\n",
    "\n",
    "for s in sentences:\n",
    "   doc = best_nlp(s)\n",
    "   displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b261d9fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
